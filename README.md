# CS412Project

**Group members:** Arda Berk Çetin (31077), Ece Güler (29107)

# Approaches:

Our project aimed to discover correlations between student interactions with ChatGPT and their assignment grades. By analyzing dialogue patterns, we aim to uncover insights that correlate with academic performance. To achieve this, we embarked on a comprehensive process involving data preprocessing, feature engineering, sentiment analysis, and advanced machine learning techniques. Here's a detailed overview of our approaches:

To find a possible relation between ChatGPT prompts and grades, we applied a multitude of approaches and made sometimes quite minor, and sometimes, major changes to the document.

One of the minor changes we did is that we removed markdown from the question prompts to facilitate prompt matching, hypothesizing that markdown's absence indicates a more focused query. Since markdown is not copied, this might facilitate prompt matching with questions. Another minor change we did is that we added some more keywords to keywords2search. Enhanced keywords2search with additional terms like "code here", "pts", "instead", "try", "why", "hint", "Traceback", "most recent call last", "google", "http", and "driver". Each keyword was chosen based on its potential indication of user engagement or specific query types. Description for why we added these are as follows:

"code here", "pts": This is a part of the question that is not really needed to be copied since it gives no additional info to ChatGPT. We thought this might indicate sloppiness on user's end and hence, a lower grade.

"instead", "try", "why", "hint": This might signal the user is dissatisfied with the result and confused about ChatGPT's response. 

"Traceback", "most recent call last": This indicates that the user encountered and error and directly copy pasted the error message into ChatGPT.

"google", "http": This might indicate the user googled something or posted a link after it was dissatisfied with the answer.

"driver": We added this to see if the user asked about driver features.

These keyword analyses were crucial for understanding the students' interactions and their level of engagement with the AI. They also helped us spot trends in how students behaved, which gave us clues about their understanding and effort.In addition to these keyword-based insights, we delved deeper into the emotional context of the user's messages.

On top of these changes, an important approach we took is that we applied sentiment analysis on every prompt of the user to possibly find a relation with grades. This seemed logical to us since the user might react negatively if ChatGPT is consistently returning useless answers. Conversely, if ChatGPT is returning useful answers, the user might react positively. The model we used also had neutral sentiment which might still be useful in comparison to its balance with negative and positive sentiments. We downloaded a pretrained model from Huggingface, namely the famous RoBERTa. Applied sentiment analysis on each user prompt to gauge emotional responses, using the RoBERTa model for comprehensive sentiment assessment. This analysis aimed to correlate sentiment with assignment scores, hypothesizing that negative sentiment might indicate lower grades due to unsatisfactory interactions. This helped us understand the emotional tone in student responses and potentially correlate it with assignment scores. It provided us with a deeper layer of understanding, allowing us to infer student satisfaction and frustration levels, which could be key indicators of their learning process. Then, we calculated the average negative, neutral and positive sentiment for each user using RoBERTa.

Another important approach we took is that we scraped the date on every document. This was a challenging task because the date information is generated by javascript. To effectively extract date information from the chats, we encountered the challenge of scraping JavaScript-rendered content. Traditional methods like BeautifulSoup were insufficient for this task, leading us to implement Selenium for web scraping. This tool was crucial for dealing with the dynamic elements of the web pages, allowing us to successfully gather important date information from the chats. This aspect of data collection was key in adding a time-related perspective to our analysis. Since the latest Chromedriver is for Chrome version 114 and we were using Chrome version 120, we had to delete and redownload Chrome. Then, we finally managed to scrape the date information which was added as a feature after representing it as the difference from the latest date. This way was particularly crucial for retrieving dates or other data from chats.

Then we applied another minor change. We multiplied the probabilities from quesion-prompt matching with the points of the questions to take their different weights. Afterall, not every question was of equal importance. This method made sure our model understood the importance of different types of interactions, leading to more accurate predictions.

Finally, the last important approach we took is that we trained two neural networks. The first neural network does binary classification. Its goal is to correctly classify grades that are in the bottom 25th percentile of grades. This is done separately due to the high right-skewed nature of our data. Then, the predictions of this specialized neural network is fed to the second one in combination with the all other features we were already using. The second neural network than outputs a grade prediction between 0 and 100. This is achieved by having an ultimate layer that multiplies the outcome of the penultimate sigmoid layer by 100. Note that we made sure to not accidentally train our first neural network (classifier) with grade information and our second neural network (grade predictor) with the actual low grade information. Our two-model strategy effectively handled the uneven nature of our data, enabling our model to make accurate predictions for both low and high grades.

To make sure the train/test split was done accurately, we made sure the ratio of low grades in both the train and test set was close to each other by repeatedly splitting until we got a good result. We would have used something like StratifiedShuffleSplit or simply add "stratify=y" to train_test_split. However, because we had so little data, we didn't have enough different y values to make this work. Nevertheless, we managed to split our data evenly. Achieving a balanced division between training and testing data was key for accurately assessing our model. We carefully checked that the grades in both the training and test sets reflected the overall distribution of grades in the complete dataset. This careful balancing was important due to the uneven distribution of grades and the small size of our dataset, helping to avoid biased predictions.

Lastly, we also tried training a random forest classifier with GridCV to find the best hyperparameters. Alongside our advanced neural network models, we also experimented with random forest classifiers, known for their capability with varied data. We used GridSearchCV for an extensive search to find the best settings, aiming to improve the model's pattern recognition and predictive power. Our approach focused on refining our models to ensure strong and dependable predictions. We continually trained, tested, and adjusted the models, aiming for the most accurate and general results possible from our data.

# Results:

Long story short, we didn't manage to find a relation.

Our binary classifier has the following confusion matrix:

![Confusion Matrix](https://github.com/ArdaBC/FearlessPianoPancake/blob/main/confusion_matrix.png?raw=true)

As you can see, this is not that impressive, but still, not horrible. It has an accuracy of 72% on the test set. Now, we feed the predictions from this outlier classifier to the neural network and we get the following result:

![NN Results](https://github.com/ArdaBC/FearlessPianoPancake/blob/main/nnresults.png?raw=true)

This seems okay, however, when you take a close look at the predictions of our neural network, the allure is lost:

Index | Actual | Predicted
--- | --- |---
0 | 97.0 | 99.966873 
1 | 96.0 | 99.985054
2 | 93.0 | 99.999931
3 | 90.0 | 97.460190
4 | 98.0 | 98.051086
5 | 99.0 | 99.998169
6 | 100.0 | 100.000000
7 | 100.0 | 99.999817
8 | 100.0 | 99.999947
9 | 88.0 | 100.000000
10 | 92.0 | 100.000000
11 | 98.0 | 99.999695
12 | 99.0 | 100.000000
13 | 89.0 | 99.999878
14 | 94.0 | 100.000000
15 | 96.0 | 99.999985
16 | 100.0 | 100.000000
17 | 97.0 | 100.000000
18 | 99.0 | 100.000000
19 | 97.0 | 99.995987
20 | 84.0 | 100.000000
21 | 90.0 | 99.989761
22 | 15.0 | 100.000000
23 | 78.0 | 99.993286
24 | 94.0 | 99.999969

Our model has low error because it predicts high grades for everyone, and it gets away with this because the data is extremely right skewed. Furthermore, as can be seen on the 22nd line, our model predicted that someone who got 15 got 100. This shows that our approach of using two neural networks was not useful.

If we look at the results of our random forest classifier, we also see nothing useful unfortunately. With a max depth of 5, minimum samples per leaf 2, minimum samples per split 5 and 10 estimators, we got the following results:

MSE Train: 45.45360824742268

R2 Train: 0.628097499043445

MSE Test: 339.12

R2 Test: -0.24650074101954877

This tells us our model did a poor job.

Lastly, let's take a look at the decision tree results:

MSE Train: 7.168113225486601

MSE TEST: 389.7955068818447

R2 Train: 0.941350327543042

R2 TEST: -0.4327683067183006

This is clearly a case of overfitting. R2 and MSE on training data is good, but we are doing horribly on test data. Therefore, this is not useful either.

The project highlighted various challenges, particularly in preprocessing steps and the feature extraction process. Maintaining model generalization without overfitting was a critical consideration. Our project represented a comprehensive effort to predict student performance based on AI interactions. Despite facing difficulties and getting mixed results, we learned important things about using language technology in education.

To sum it up, our project explored predicting how students would do based on AI interactions. We used different models and methods to understand the connections between student questions and grades. This helped us learn more about using language technology in education. While we didn't find a direct connection, our work sets the stage for future research and progress in this field.

