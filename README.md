# CS412Project

**Group members:** Arda Berk Çetin (31077), Ece Güler (29107)

# Approaches:

To find a possible relation between ChatGPT prompts and grades, we applied a multitude of approaches and made sometimes quite minor, and sometimes, major changes to the document.

One of the minor changes we did is that we removed markdown from the question prompts. Since markdown is not copied, this might facilitate prompt matching with questions. Another minor change we did is that we added some more keywords to keywords2search. Description for why we added these are as follows:

"code here", "pts": This is a part of the question that is not really needed to be copied since it gives no additional info to ChatGPT. We thought this might indicate sloppiness on user's end and hence, a lower grade.

"instead", "try", "why", "hint": This might signal the user is dissatisfied with the result and confused about ChatGPT's response. 

"Traceback", "most recent call last": This indicates that the user encountered and error and directly copy pasted the error message into ChatGPT.

"google", "http": This might indicate the user googled something or posted a link after it was dissatisfied with the answer.

"driver": We added this to see if the user asked about driver features.

On top of these changes, an important approach we took is that we applied sentiment analysis on every prompt of the user to possibly find a relation with grades. This seemed logical to us since the user might react negatively if ChatGPT is consistently returning useless answers. Conversely, if ChatGPT is returning useful answers, the user might react positively. The model we used also had neutral sentiment which might still be useful in comparison to its balance with negative and positive sentiments. We downloaded a pretrained model from Huggingface, namely the famous RoBERTa. Then, we calculated the average negative, neutral and positive sentiment for each user using RoBERTa.

Another important approach we took is that we scraped the date on every document. This was a challenging task because the date information is generated by javascript. Therefore, it could not be scraped with BeautifulSoup. So we had to resort to using Selenium. Since the latest Chromedriver is for Chrome version 114 and we were using Chrome version 120, we had to delete and redownload Chrome. Then, we finally managed to scrape the date information which was added as a feature after representing it as the difference from the latest date.

Then we applied another minor change. We multiplied the probabilities from quesion-prompt matching with the points of the questions to take their different weights. Afterall, not every question was of equal importance.

Finally, the last important approach we took is that we trained two neural networks. The first neural network does binary classification. Its goal is to correctly classify grades that are in the bottom 25th percentile of grades. This is done separately due to the high right-skewed nature of our data. Then, the predictions of this specialized neural network is fed to the second one in combination with the all other features we were already using. The second neural network than outputs a grade prediction between 0 and 100. This is achieved by having an ultimate layer that multiplies the outcome of the penultimate sigmoid layer by 100. Note that we made sure to not accidentally train our first neural network (classifier) with grade information and our second neural network (grade predictor) with the actual low grade information. 

To make sure the train/test split was done accurately, we made sure the ratio of low grades in both the train and test set was close to each other by repeatedly splitting until we got a good result. We would have used something like StratifiedShuffleSplit or simply add "stratify=y" to train_test_split. However, because we had so little data, we didn't have enough different y values to make this work. Nevertheless, we managed to split our data evenly.

Lastly, we also tried training a random forest classifier with GridCV to find the best hyperparameters.

# Results:

:(
